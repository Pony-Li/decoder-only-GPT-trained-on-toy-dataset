# decoder only GPT trained on toy dataset

This repository is a detailed implementation of Andrej Karpathy's video on youtube(https://www.youtube.com/watch?v=kCc8FmEb1nY).

"We build a Generatively Pretrained Transformer (GPT), following the paper "Attention is All You Need" and OpenAI's GPT-2 / GPT-3. We talk about connections to ChatGPT, which has taken the world by storm. We watch GitHub Copilot, itself a GPT, help us write a GPT (meta :D!) . I recommend people watch the earlier makemore videos to get comfortable with the autoregressive language modeling framework and basics of tensors and PyTorch nn, which we take for granted in this video."
                                                                                                                                                            ---Andrej Karpathy
### code structure

See [gpt-dev.ipynb](./gpt-dev.ipynb) for detailed imformation on how to train a decoder-only GPT on a toy dataset([tinyShakespeare.txt](./tinyShakespeare.txt)).
