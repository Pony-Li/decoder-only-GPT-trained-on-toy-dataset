{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick recognition of our training text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115393\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read it to inspect our training text\n",
    "with open('tinyShakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"length of dataset in characters: \", len(text))\n",
    "\n",
    "# let's look at the first 1000 characters\n",
    "print(text[:1000])\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i, ch in enumerate(chars) }\n",
    "itos = { i:ch for i, ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115393]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) target is 47\n",
      "when input is tensor([18, 47]) target is 56\n",
      "when input is tensor([18, 47, 56]) target is 57\n",
      "when input is tensor([18, 47, 56, 57]) target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) target is 58\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[53, 59,  6,  1, 58, 56, 47, 40],\n",
      "        [49, 43, 43, 54,  1, 47, 58,  1],\n",
      "        [13, 52, 45, 43, 50, 53,  8,  0],\n",
      "        [ 1, 39,  1, 46, 53, 59, 57, 43]])\n",
      "target:\n",
      "torch.Size([4, 8])\n",
      "tensor([[59,  6,  1, 58, 56, 47, 40, 59],\n",
      "        [43, 43, 54,  1, 47, 58,  1, 58],\n",
      "        [52, 45, 43, 50, 53,  8,  0, 26],\n",
      "        [39,  1, 46, 53, 59, 57, 43,  0]])\n",
      "------------------------------------------------------\n",
      "when input is [53] the target: 59\n",
      "when input is [53, 59] the target: 6\n",
      "when input is [53, 59, 6] the target: 1\n",
      "when input is [53, 59, 6, 1] the target: 58\n",
      "when input is [53, 59, 6, 1, 58] the target: 56\n",
      "when input is [53, 59, 6, 1, 58, 56] the target: 47\n",
      "when input is [53, 59, 6, 1, 58, 56, 47] the target: 40\n",
      "when input is [53, 59, 6, 1, 58, 56, 47, 40] the target: 59\n",
      "when input is [49] the target: 43\n",
      "when input is [49, 43] the target: 43\n",
      "when input is [49, 43, 43] the target: 54\n",
      "when input is [49, 43, 43, 54] the target: 1\n",
      "when input is [49, 43, 43, 54, 1] the target: 47\n",
      "when input is [49, 43, 43, 54, 1, 47] the target: 58\n",
      "when input is [49, 43, 43, 54, 1, 47, 58] the target: 1\n",
      "when input is [49, 43, 43, 54, 1, 47, 58, 1] the target: 58\n",
      "when input is [13] the target: 52\n",
      "when input is [13, 52] the target: 45\n",
      "when input is [13, 52, 45] the target: 43\n",
      "when input is [13, 52, 45, 43] the target: 50\n",
      "when input is [13, 52, 45, 43, 50] the target: 53\n",
      "when input is [13, 52, 45, 43, 50, 53] the target: 8\n",
      "when input is [13, 52, 45, 43, 50, 53, 8] the target: 0\n",
      "when input is [13, 52, 45, 43, 50, 53, 8, 0] the target: 26\n",
      "when input is [1] the target: 39\n",
      "when input is [1, 39] the target: 1\n",
      "when input is [1, 39, 1] the target: 46\n",
      "when input is [1, 39, 1, 46] the target: 53\n",
      "when input is [1, 39, 1, 46, 53] the target: 59\n",
      "when input is [1, 39, 1, 46, 53, 59] the target: 57\n",
      "when input is [1, 39, 1, 46, 53, 59, 57] the target: 43\n",
      "when input is [1, 39, 1, 46, 53, 59, 57, 43] the target: 0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for prediction?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # 生成 batch_size 个随机起始索引, len(data) - block_size 确保 x 取的范围不会超出数据长度\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # shape of x: (batch_size, block_size)\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # shape of y: (batch_size, block_size)\n",
    "    return x, y \n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('target:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('------------------------------------------------------')\n",
    "\n",
    "for b in range(batch_size): #batch dimension\n",
    "    for t in range(block_size): #time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[53, 59,  6,  1, 58, 56, 47, 40],\n",
      "        [49, 43, 43, 54,  1, 47, 58,  1],\n",
      "        [13, 52, 45, 43, 50, 53,  8,  0],\n",
      "        [ 1, 39,  1, 46, 53, 59, 57, 43]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # our input to the transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple Bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8948, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# 每次仅仅根据前一个token来预测下一个token的内容\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads of logits for next token in a lookup tabel\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        '''\n",
    "        在 PyTorch 中, nn.Embedding(d1, d2) 创建了一个查找表 (lookup table), 用于将离散索引 (token ID) 映射到高维向量。\n",
    "        其中包含 d1 个向量，每个向量的维度是 d2. 这些向量的初始值是随机的, 并且符合标准正态分布的一种变体。\n",
    "        '''\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C), where C is vocab_size = 65\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets) # F.log_softmax(logits, dim=-1)：对 logits 进行 softmax 归一化，并取对数\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions, where idx is (B, C) dimension\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # 按照 probs 的概率大小，随机选取 num_samples 个样本, shape: (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "# 当预测完全随机时 loss = log(1/65)\n",
    "\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4699912071228027\n"
     ]
    }
   ],
   "source": [
    "# create a Pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)\n",
    "\n",
    "# train the biagram model\n",
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "\n",
    "  # sample a batch of data\n",
    "  xb, yb = get_batch('train')\n",
    "\n",
    "  # evaluate the loss\n",
    "  logits, loss = m(xb, yb)\n",
    "  optimizer.zero_grad(set_to_none=True) # 清空梯度，防止梯度累积\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iyoteng h hasbe pave pirance\n",
      "Rie hicomyonthar's\n",
      "Plinseard ith henouratucenonthioneir thondy, y heltieiengerofo'dsssit ey\n",
      "KIN d pe wither vouprrouthercc.\n",
      "hathe; d!\n",
      "My hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so it t jod weancotha:\n",
      "h haybet--s n prids, r loncave w hollular s O:\n",
      "HIs; ht anjx?\n",
      "\n",
      "DUThinqunt.\n",
      "\n",
      "LaZAnde.\n",
      "athave l.\n",
      "KEONH:\n",
      "ARThanco be y,-hedarwnoddy scat t tridesar, wnl'shenou\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(m.generate(idx, max_new_tokens=400)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The mathematiacal trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n",
      "tensor([[[ 0.1808, -0.0700],\n",
      "         [-0.3596, -0.9152],\n",
      "         [ 0.6258,  0.0255],\n",
      "         [ 0.9545,  0.0643],\n",
      "         [ 0.3612,  1.1679],\n",
      "         [-1.3499, -0.5102],\n",
      "         [ 0.2360, -0.2398],\n",
      "         [-0.9211,  1.5433]],\n",
      "\n",
      "        [[ 1.3488, -0.1396],\n",
      "         [ 0.2858,  0.9651],\n",
      "         [-2.0371,  0.4931],\n",
      "         [ 1.4870,  0.5910],\n",
      "         [ 0.1260, -1.5627],\n",
      "         [-1.1601, -0.3348],\n",
      "         [ 0.4478, -0.8016],\n",
      "         [ 1.5236,  2.5086]],\n",
      "\n",
      "        [[-0.6631, -0.2513],\n",
      "         [ 1.0101,  0.1215],\n",
      "         [ 0.1584,  1.1340],\n",
      "         [-1.1539, -0.2984],\n",
      "         [-0.5075, -0.9239],\n",
      "         [ 0.5467, -1.4948],\n",
      "         [-1.2057,  0.5718],\n",
      "         [-0.5974, -0.6937]],\n",
      "\n",
      "        [[ 1.6455, -0.8030],\n",
      "         [ 1.3514, -0.2759],\n",
      "         [-1.5108,  2.1048],\n",
      "         [ 2.7630, -1.7465],\n",
      "         [ 1.4516, -1.5103],\n",
      "         [ 0.8212, -0.2115],\n",
      "         [ 0.7789,  1.5333],\n",
      "         [ 1.6097, -0.4032]]])\n"
     ]
    }
   ],
   "source": [
    "# consider the fllowing toy example\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8 ,2 # batch, time channels\n",
    "x = torch.randn(B, T, C)\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]])\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    }
   ],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "# version 1\n",
    "xbow = torch.zeros(B, T, C)\n",
    "for b in range(B):\n",
    "  for t in range(T):\n",
    "    xprev = x[b, :t+1] #(t,C)\n",
    "    xbow[b,t] = torch.mean(xprev, 0)\n",
    "\n",
    "print(x[0])\n",
    "print(xbow[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# 前面的代码实现了沿着某个维度对张量求平均的功能, 虽然满足要求但是效率较低\n",
    "# 下面使用矩阵乘法来高效实现该功能\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    }
   ],
   "source": [
    "# version 2\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "\n",
    "print(xbow[0])\n",
    "print(xbow2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    }
   ],
   "source": [
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "print(wei)\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "print(wei)\n",
    "xbow3 = wei @ x\n",
    "\n",
    "print(xbow[0])\n",
    "print(xbow3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0248, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0052, 0.0091, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0521, 0.0135, 0.2482, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3171, 0.0214, 0.1642, 0.1188, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0412, 0.0487, 0.1046, 0.0742, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1060, 0.5347, 0.2059, 0.1030, 0.7402, 0.0192, 0.0000, 0.0000],\n",
      "         [0.4298, 0.3409, 0.1769, 0.2027, 0.0480, 0.8472, 0.2329, 0.0000],\n",
      "         [0.0238, 0.0316, 0.1002, 0.5013, 0.0117, 0.1336, 0.7671, 1.0000]],\n",
      "\n",
      "        [[0.0443, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0042, 0.0375, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0560, 0.0210, 0.2496, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3679, 0.1441, 0.4929, 0.0438, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0088, 0.1052, 0.0604, 0.5847, 0.2046, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0367, 0.0895, 0.0362, 0.2074, 0.1029, 0.0115, 0.0000, 0.0000],\n",
      "         [0.0480, 0.5010, 0.0172, 0.1434, 0.2807, 0.7090, 0.7318, 0.0000],\n",
      "         [0.4341, 0.1018, 0.1437, 0.0206, 0.4118, 0.2794, 0.2682, 1.0000]],\n",
      "\n",
      "        [[0.0419, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0679, 0.0901, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0119, 0.0392, 0.1158, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0041, 0.5063, 0.1163, 0.1399, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.7491, 0.0460, 0.2084, 0.0659, 0.0292, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0583, 0.1241, 0.2200, 0.0712, 0.2419, 0.1883, 0.0000, 0.0000],\n",
      "         [0.0107, 0.1200, 0.2721, 0.6404, 0.5979, 0.7420, 0.9713, 0.0000],\n",
      "         [0.0562, 0.0744, 0.0674, 0.0826, 0.1310, 0.0697, 0.0287, 1.0000]],\n",
      "\n",
      "        [[0.2196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0937, 0.0126, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0881, 0.0591, 0.0066, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0682, 0.0118, 0.0908, 0.0115, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0934, 0.0551, 0.0891, 0.1162, 0.0787, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3185, 0.6763, 0.0329, 0.3541, 0.3450, 0.1410, 0.0000, 0.0000],\n",
      "         [0.0340, 0.0079, 0.3160, 0.0306, 0.0840, 0.6004, 0.1996, 0.0000],\n",
      "         [0.0846, 0.1772, 0.4646, 0.4876, 0.4922, 0.2586, 0.8004, 1.0000]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32 # batch, time, channel\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# Let's see a single head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "v = value(x) # (B, T, 16)\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=1)\n",
    "out = wei @ v # (B, T, T) @ (B, T, 16) ---> (B, T, 16)\n",
    "\n",
    "print(wei)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # batch mean\n",
    "    xvar = x.var(1, keepdim=True) # batch variance\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder-only GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.788929 M parameters\n",
      "step 0: train loss 4.2846, val loss 4.2823\n",
      "step 500: train loss 1.8772, val loss 1.9839\n",
      "step 1000: train loss 1.5299, val loss 1.7114\n",
      "step 1500: train loss 1.3905, val loss 1.6044\n",
      "step 2000: train loss 1.3086, val loss 1.5457\n",
      "step 2500: train loss 1.2477, val loss 1.5085\n",
      "step 3000: train loss 1.2049, val loss 1.4964\n",
      "step 3500: train loss 1.1607, val loss 1.4806\n",
      "step 4000: train loss 1.1220, val loss 1.4766\n",
      "step 4500: train loss 1.0885, val loss 1.4809\n",
      "\n",
      "Thy women divorces with me,\n",
      "Whethough gross than it is off?\n",
      "\n",
      "AUFIDIUS:\n",
      "In go alive,\n",
      "Without lack more passatives; bore of my grave,\n",
      "Though they do perhappy heart.\n",
      "Here husband, and Bona: thou wish them side;\n",
      "He may mercy coloure pleaset me, though I shall not\n",
      "Choose my sacred be not a man interchange?\n",
      "Nay, if I never be,\n",
      "And Messfreth, one curties oft praised and to thee\n",
      "Ups of my vengeance pericion.\n",
      "\n",
      "BENVOLIO:\n",
      "Go it home too longue to grief, but lips\n",
      "That heart cried ways and no ward:--whom mars, do smeal\n",
      "To thy gold good success add and 'pardoning go.\n",
      "\n",
      "EXETER:\n",
      "Speak,\n",
      "Could speak welcome lost me; man, I prithee another\n",
      "Not live, with move times lions: speak wail, I'll bear\n",
      "Could you thank you to keep wear.\n",
      "\n",
      "KING EDWARD IV:\n",
      "No; let me go suspicious ill some,\n",
      "And put quenner's name. Come, come both  on my spirt:\n",
      "Peach crims, and afterward preplected to smoke,\n",
      "Wheel them posses where I divided the bed by my king,\n",
      "Hath buried 'press'd by mother true:' bear\n",
      "In Thursday the inough, I will it dare.\n",
      "\n",
      "JULIET:\n",
      "Will take move me the late enduct most follow,\n",
      "To seem the conquire of your state; but who meant\n",
      "They may do dead an end vileIal pity\n",
      "He should sit to Florizense. This is it to desire\n",
      "To ruther get them on peeds and greeten:\n",
      "And them that I hyll conquest.\n",
      "\n",
      "JULIET:\n",
      "I would thou wert swear the I wood upon me;\n",
      "Not all that beggar, as thou didst not farewell.\n",
      "How thou pleaset thoughtst strike most a king?\n",
      "Do thou have new think'st thou wouldst have write sorrow?\n",
      "And do I tainted, plain that I would the formey\n",
      "To do do't. Come, are thou determined.\n",
      "Thou everm'st chaste a hair and shaped 'awe!\n",
      "Why do the issue that be hath old me waked\n",
      "You without worthy testures to this own condemptro\n",
      "From I enjoy'd thee, you take overded in death!\n",
      "My uncle, man: in mightI should and such elderfol\n",
      "Is first my ingraced son, yet on the crown?\n",
      "\n",
      "CORIOLANUS:\n",
      "To mean to him,\n",
      "And I have repeal'd my detected by richarity.\n",
      "Thou art all thy Norfolk, not attainted one,\n",
      "Thou comfort me to weeping, but the more,\n",
      "Nor be bade mades, thou daring there will not\n",
      "Upen thy rogue-washion, wore I'll sendure others.\n",
      "\n",
      "First Gentleman:\n",
      "Each more I pleasure than to thy griefs,\n",
      "Nor in thee an othen all Frtarlis work!\n",
      "\n",
      "Third Servingman:\n",
      "He seems I kning Menenius,\n",
      "And prosperously to heaven black thy prince,\n",
      "Abave of thy labour topbered,\n",
      "Send thy dark place for his forbidder daughter.\n",
      "\n",
      "EDWARD:\n",
      "So shall how fares too? now Marcius Arial and God,\n",
      "His kingler Henry's grief sleep me, I will\n",
      "He will comfort by his unclestern words;'\n",
      "Thyself mother beauteous worse to troops:\n",
      "Thy father cannot comfort thy house is place.\n",
      "\n",
      "KING RICHARD II:\n",
      "May demerril, every think thou, for straitors,\n",
      "And greeing royal suits my cried happy to his,\n",
      "That small art madness riberal horse that once.\n",
      "\n",
      "KING RICHARD II:\n",
      "Good unto your grace! Well, well, aunt, behold me!\n",
      "\n",
      "THOMAS MONES:\n",
      "If true bodil to greetings. Tell me, get them forgive\n",
      "Known to our ministery; my name.\n",
      "But farines our act our such is not to you;\n",
      "That great dath you that yet prect me.\n",
      "\n",
      "RICHARD:\n",
      "Your tent, let it go go: the hay\n",
      "Was little can but you, so feeling: for the diving it,\n",
      "To in this ceeven plent of great enemies!\n",
      "And therefore facles mine he is not fayl.\n",
      "Here come, and with Mowbray sometimes are,\n",
      "What flower Richard Gentleman?\n",
      "\n",
      "BUCKINGHAM:\n",
      "We make determinate my lord, in reverence our judge,\n",
      "Breaking cover.\n",
      "\n",
      "KING RICHARD III:\n",
      "Here, letter. Drink, we will. For the chose.\n",
      "\n",
      "WATHTHASTINGRE:\n",
      "My gracious lord, to London for you in the beggar.\n",
      "Wi can it Hereford your judge's instance,\n",
      "And sight upon me, good my lady:\n",
      "Yet have done accused; with tormedly soil:\n",
      "Thou of the litter pardon of me; for thou last:\n",
      "I'll take cry Claud offord me; as well, speak of thee\n",
      "Appeak words.\n",
      "\n",
      "AUTOLYCUS:\n",
      "Friend this is that sense the clouds up DukFlUNUS:\n",
      "He slegs not on his skissaring near\n",
      "In such man, you have been the like\n",
      "Which spoil with summers as I gave.\n",
      "\n",
      "CARLISLO:\n",
      "Why, is it you?\n",
      "Here come now? why not your husband?\n",
      "As do I heard, go you gone, provost. But, let am me\n",
      "To uponder her with no sob arrivable.\n",
      "\n",
      "JULIET:\n",
      "You will not, pips the gone.\n",
      "\n",
      "Second Consper:\n",
      "But, give me return witle pluck a girl:\n",
      "Shametier high, and my son of Richard so heaven\n",
      "His cours and hoster pirates, cup!\n",
      "\n",
      "SICINIUS:\n",
      "How shall I, your wife with a tongue call'd make\n",
      "That prevestern'd honour, as she's sweet breath?\n",
      "The rest, who though shep not other they breath?\n",
      "\n",
      "VOLUMNIA:\n",
      "Nay, being gone, God-day, and heart\n",
      "When that fast over-rie-glorious sceeding powerful\n",
      "Screators. We have shed lack the blanker out\n",
      "To Romeo's earth! Come boy, when, Clifford,\n",
      "Out our minous 'voke heir;' the cleased means\n",
      "Did lieute of us, have says damptation for Clauge,\n",
      "The wape, and the proflity days have no furmed meanles,\n",
      "But soldier born upon us and course us,\n",
      "And nothing for I think on my super,\n",
      "Presump'd beaution, when thou do not know thee\n",
      "Against those busining worth to remortal with\n",
      "vines: but in merits this invinchant more beware,\n",
      "To make for our suit in hell. He art at ourst thy news\n",
      "Doth-pale duke't!\n",
      "And who sit by my sight for theme take\n",
      "Lod on my worst circummentance but them to seve\n",
      "Showning here fruit me by this was enave our affairs\n",
      "Shall before it.\n",
      "\n",
      "SICINIUS:\n",
      "You were no doubt in your enemy! I heard them,\n",
      "And only churchions and so revellenge\n",
      "As to pitch him find you slept.\n",
      "How say you, sie, couldst me you, and regreet this\n",
      "faults yet a second man for love Belia's graves to hope;\n",
      "For your hats to bed, their side from them for himself.\n",
      "\n",
      "CAPULET:\n",
      "If he speaks me, leave she is kind that part, let's\n",
      "Too try and pawn from yours of our royal counsel is\n",
      "Nor proping.\n",
      "\n",
      "LADY CAPULET:\n",
      "She's an obadier,\n",
      "Dear knocks with greatest than the bodes\n",
      "Reign on the phygmast enougned mine and me\n",
      "And fly that now far sorrows, which he speaks I know.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "She, as one aft the way, they shall flesh they last.\n",
      "\n",
      "HENRY PERcessman: but so ease themselves\n",
      "Along, but to all the death--\n",
      "For what you repared may persuove twice\n",
      "Her tongue, make your kind, my son with you disdain:\n",
      "The right wenches of the ground hand on that he rave\n",
      "Till the sun of Apriar fortune can, the love\n",
      "Were exiled friend of our princes,\n",
      "Whither my bodies your anger-ence? with youth a man to my countrymen,\n",
      "Not in his youth observate in the sudden's thee!\n",
      "What is as yeme-king and what wisvice?\n",
      "\n",
      "BUCKINGHAM:\n",
      "The belly one state to your heart-swords\n",
      "That grieve his innocency can let his hand:\n",
      "What my unbruish'd with death of my lance,\n",
      "And must tell me on sogfully withan oath.\n",
      "Lay the more aumpire had done! once farewell,\n",
      "That makest, to our beautingness made impative,\n",
      "Wondom what calls from her I am lord'd,\n",
      "His deed furself and their charge wheeds they revenged\n",
      "To prevents, make bround in King Richard's eye,\n",
      "I indeed, do not bechance they are married;\n",
      "Thus not Romeo, adopt us, they say I' the solity.\n",
      "\n",
      "RICHARD:\n",
      "Northumbering that in my own good rage\n",
      "And shall fool living it unblest your grace.\n",
      "\n",
      "PRINCE:\n",
      "Not beliave of my deen, but yet my prove fount.\n",
      "\n",
      "GRUMTRUCHARD IIIO:\n",
      "A merit!\n",
      "You shall turn not breath that halls what I abiliary.\n",
      "\n",
      "KING RICHARD III:\n",
      "Sweet with Peace of Marshal?\n",
      "\n",
      "DERBY:\n",
      "O, then Daughter, then shall pour debt, sour patish hand.\n",
      "\n",
      "GREY:\n",
      "I was very winto hear too here, and go before;\n",
      "Pray the canon that would spare yet stir fall a grave?\n",
      "A twick-tick-walk, a worder'd father officers:\n",
      "No, go with your heart as forwards Citizens:\n",
      "I wishing 'pos!\n",
      "\n",
      "TRANIO:\n",
      "What, that did I love thee? Is not thee?\n",
      "\n",
      "PETRUCHIOLANUS:\n",
      "Marry, sir, God my tongue,\n",
      "Are you heard prove them to me; see't, I would be not dead\n",
      "Unto you warrant, fairly susposit, word, they shall\n",
      "Custom them wast monthy bire much attempt.\n",
      "Thou hast Romeo--woful, where we may, I'll pleased?\n",
      "\n",
      "LADY GREY:\n",
      "'Twounds in Warwick, she had knows the earlialteth,\n",
      "And here it say all my lord; do noti my loyalty,\n",
      "But to are it at ire, all would the rude means\n",
      "appointed natural gried with the lust blaws,\n",
      "And much more to thine loud o'inger suoulder,\n",
      "Nurse the wounder, which could do pass disvate.\n",
      "\n",
      "LEONTES:\n",
      "The gods for that Biohnminato, and let's ripen,\n",
      "Unto their bitters bearing out best and press!'\n",
      "This exile to addemption might be doubted power,\n",
      "Which will command enjoy burth, relit up,\n",
      "For bid rage canning her, can Lucely,\n",
      "As becomest upon the all of a damnable as\n",
      "Fall as purplacles: death, I not never will hang bear\n",
      "The golden consume to prisoner and be charity.\n",
      "O, the mista year bargue sent not away!\n",
      "Would I armuled hand! faintain weep,\n",
      "Both to breaking the rebel world of this womary,\n",
      "They agre-court: why, most sons them Cage Master Blunt,\n",
      "Like himondy to a wanton him action\n",
      "That find my caparting lold dear member.\n",
      "\n",
      "KING RICHARD II:\n",
      "Why stay not, for this I do well. But, that dare agrieves\n",
      "But have prepaties the basily made\n",
      "To sleep me some rich to tongues have you,\n",
      "To use thyself men, we'll to set the part of him:\n",
      "Betwixt he, here is no steply; and thy tenting in\n",
      "An all there-perdined by tunes, here own dear load\n",
      "By lord you will follow here this marchanifly.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "The unsulvants I lion or tell\n",
      "The move is true third--\n",
      "\n",
      "KING RICHARD III:\n",
      "Ay, well the deep grieve and Warwick's book,\n",
      "And now that possed to thyself thee my head.\n",
      "\n",
      "JULIET:\n",
      "Art thou told me, daughter; for I had thou and want thyself\n",
      "To littled what I creet against myself!\n",
      "Shalt state betwixt thy mother will I prove:\n",
      "Some of those lips both wife these murders' tears,\n",
      "Heir virtue untrain committings where I am,\n",
      "But one of a cause of kings, who after toy;\n",
      "And thou in most doubt thee to kissive\n",
      "Thy serving wreatht! You kill thy hazard it,\n",
      "Are never to my yet sort blood?\n",
      "\n",
      "SITANLER:\n",
      "Will I tell you guess.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "'Tis twenty deep boy; nay, for he breatheth,\n",
      "Sir, and will never had together be verenced.\n",
      "\n",
      "GLOUCESTER:\n",
      "And then sure when it was sleep, which he\n",
      "We meet you have puts jectly offered\n",
      "Thy firm up our own point. Come, get your fires,\n",
      "I go thee in nose, that were my worthy cungeal.\n",
      "Fever justice in province himself of thy silver!\n",
      "With a booking any gmafter's sight-ktotted gnarding,\n",
      "for not myself\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # to run on a GPU if you have it\n",
    "eval_iters = 200\n",
    "n_embd = 384 # number of embedding dimension\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('tinyShakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars) # vocab_size = 65\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # 生成 batch_size 个随机起始索引, len(data) - block_size 确保 x 取的范围不会超出数据长度\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # shape of x: (batch_size, block_size)\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # shape of y: (batch_size, block_size)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y) # foward 函数除了能前向传播输出结果, 还能够计算并返回 loss\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# one head of self-attention\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    # self-attention 的核心思想是用 key 和 query 的内积来表征不同 token 之间的\"匹配度\",\n",
    "    # 再以这个\"匹配度\"为权重计算 value 的加权和作为 self-attention 的最终输出结果。\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # n_embd=384, head_size=64, num_heads=6\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # (T, T) 下三角阵\n",
    "        '''\n",
    "        在 torch.nn.Module 中, register_buffer 允许你注册不作为参数更新的变量(即不参与梯度计算),\n",
    "        但仍会被存储在模型的 state_dict 中，并随模型一起移动到 GPU/CPU。\n",
    "        '''\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout) # 创建一个 Dropout 层, 在训练时随机将部分权重置为零。作用是防止过拟合, 提高模型泛化能力。\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        '''\n",
    "        k.shape[-1] 就是 head_size, 当 head_size 维度很大时, q @ k^T 的值可能很大,\n",
    "        为了防止 softmax 输出过于极端, 在 Transformer 论文中, 注意力分数会进行缩放\n",
    "        '''\n",
    "\n",
    "        # 通过下面的语句将 wei 转化为下三角矩阵, 实现 masked self-attention 功能\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei) \n",
    "        '''\n",
    "        dropout=0.2, 在训练时每个元素有 20% 概率被置为 0, 其余 80% 的值会放大 1/0.8=1.25 倍, 以保持期望值不变。\n",
    "        这样做的目的是：让模型在不同的训练阶段不会过度依赖某些特定的权重，而是学会利用所有的输入信息。\n",
    "        在测试(推理)阶段, Dropout 层将不会再起作用, 权重不会被随机置零。\n",
    "        '''\n",
    "\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd) \n",
    "        # 在 MultiHeadAttention 结构中, 需要 self.proj 这样一个全连接层, \n",
    "        # 其作用是对多个 head 计算出来的 attention 结果进行整合并映射回输入的维度。\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        # x: (B,T,C) [B=batch_size=64, T=block_size=256, C=n_embd=384]\n",
    "        # h(x): (B,T,head_size) [B=batch_size=64, T=block_size=256, head_size=64]\n",
    "        # out: (B,T,C) [B=batch_size=64, T=block_size=256, C=n_embd=384]\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head  # n-embd=384, n_head=6, head_size=64\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Residual 层（残差连接） 是一种跳跃连接（skip connection），它的作用是缓解梯度消失问题、加速训练、稳定深层网络的优化\n",
    "        # 每个Block的构成：layer norm + self-attention + residual layer + layer norm + feed foward layer + residual layer\n",
    "        # 注：Transforemer Block 中的标准化均为 layer normalization 而非 batch normalization\n",
    "\n",
    "        # x: (B,T,C) [B=batch_size=64, T=block_size=256, C=n_embd=384]\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        '''\n",
    "        *[...]表示解包 (unpacking), 用于把列表中的 Block 传递给 nn.Sequential, 相当于:\n",
    "        self.blocks = nn.Sequential(Block(...), Block(...), ..., Block(...))\n",
    "        这样就形成了一个 层叠的 Transformer 模型，即输入 x 会依次通过 n_layer=6 个 Block\n",
    "        '''\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm \n",
    "        '''\n",
    "        LayerNorm 是针对单个样本的 C 维度进行归一化;\n",
    "        BatchNorm 是针对整个 batch 在 B 维度进行归一化\n",
    "        '''\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C) [B=batch_size=64, T=block_size=256, C=n_embd=384]\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C) 这里在运算时会先将 pos_emb broadcast 为 (B, T, C) 的 Tensor\n",
    "        x = self.blocks(x) # (B,T,C) 一般情况下 self-attention block 的输入输出张量有着相同的规格\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets) # 在自回归 (auto regressive) 模型中, cross entropy loss = MLE loss\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # # 按照 probs 的概率大小，随机选取 num_samples 个样本, shape: (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device) # for using GPU\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# training loop\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))\n",
    "\n",
    "'''\n",
    "我们计算该模型的总参数个数，并给出计算公式\n",
    "1.计算 Token Embedding 层\n",
    "  模型使用 nn.Embedding(vocab_size, n_embd) 进行词嵌入:\n",
    "  参数个数=vocab_size*n_embd=65*384=24960\n",
    "\n",
    "2.计算 Position Embedding 层\n",
    "  模型使用 nn.Embedding(block_size, n_embd) 进行位置嵌入:\n",
    "  参数个数=block_size*n_embd=256*384=98304\n",
    "\n",
    "3.计算 Transformer Block(共 n_layer=6 层)\n",
    "  每个 Block 由 Multi-Head Attention 和 FeedForward 组成。\n",
    "\n",
    "  3.1 Multi-Head Attention\n",
    "      Multi-Head Attention 由 n_head=6 个 Head 组成,\n",
    "      每个 Head 具有 head_size = n_embd // n_head = 384 // 6 = 64 维度。\n",
    "      每个 Head 包含: Key (K), Query (Q), Value (V) 这三个线性变换(矩阵):\n",
    "      每个线性变换的参数个数=3*(n_embd*head_size)=3*(384*64)=73728, \n",
    "      所有 Head 总计：参数个数=6*73728=442368\n",
    "      Multi-Head Attention 的最终线性层(将拼接后的多头表示映射回 n_embd 维度)的参数个数:\n",
    "      参数个数=n_embd*n_embd=384*384=147456\n",
    "      Multi-Head Attention 总参数: 442368+147456=589824\n",
    "\n",
    "  3.2 FeedForward 层\n",
    "      包含两层全连接：\n",
    "      第一层: 输入维度=n_embd, 输出维度=4*n_embd\n",
    "      参数个数=n_embd*(4*n_embd)=384*1536=589824\n",
    "      第二层: 输入维度=4*n_embd,输出维度=n_embd\n",
    "      参数个数=(4*n_embd)*n_embd=1536*384=589824\n",
    "      FeedForward 总参数:589824+589824=1179648\n",
    "\n",
    "  3.3 LayerNorm\n",
    "      LayerNorm 有两个(不可训练)的参数: γ和β, 每个大小为 n_embd,\n",
    "      参数个数=2*n_embd=2*384=768\n",
    "      每个 Block 有 2 个 LayerNorm:\n",
    "      参数个数=768*2=1536\n",
    "\n",
    "  参数个数=589824+1179648+1536=1771008\n",
    "  6 层 Transformer的总参数:1771008*6=10626048\n",
    "\n",
    "4.计算最终 LayerNorm\n",
    "  模型的最终LayerNorm:参数个数=2*n_embd=768\n",
    "\n",
    "5.计算输出投影层\n",
    "  输出层 lm_head = nn.Linear(n_embd, vocab_size):\n",
    "  参数个数=n_embd*vocab_size=384*65=24960\n",
    "\n",
    "总参数个数=24960+98304+10626048+768+24960=10775040≈10.78M\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
